{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2321d52-7b47-486e-a938-72b560a1ac19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: import-ipynb in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (0.2)\n",
      "Requirement already satisfied: IPython in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from import-ipynb) (8.30.0)\n",
      "Requirement already satisfied: nbformat in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from import-ipynb) (5.10.4)\n",
      "Requirement already satisfied: decorator in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (2.15.1)\n",
      "Requirement already satisfied: stack-data in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (5.14.3)\n",
      "Requirement already satisfied: exceptiongroup in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (4.12.2)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (0.4.6)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from nbformat->import-ipynb) (2.21.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from nbformat->import-ipynb) (4.23.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from nbformat->import-ipynb) (5.7.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from jedi>=0.16->IPython->import-ipynb) (0.8.4)\n",
      "Requirement already satisfied: attrs>=22.2.0 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from jsonschema>=2.6->nbformat->import-ipynb) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from jsonschema>=2.6->nbformat->import-ipynb) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from jsonschema>=2.6->nbformat->import-ipynb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from jsonschema>=2.6->nbformat->import-ipynb) (0.24.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->import-ipynb) (3.10.0)\n",
      "Requirement already satisfied: pywin32>=300 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->import-ipynb) (308)\n",
      "Requirement already satisfied: wcwidth in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->IPython->import-ipynb) (0.2.5)\n",
      "Requirement already satisfied: executing in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from stack-data->IPython->import-ipynb) (0.8.3)\n",
      "Requirement already satisfied: asttokens in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from stack-data->IPython->import-ipynb) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from stack-data->IPython->import-ipynb) (0.2.2)\n",
      "Requirement already satisfied: six in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from asttokens->stack-data->IPython->import-ipynb) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install import-ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b114edf-e6a9-4939-aa2d-9be233f81601",
   "metadata": {},
   "source": [
    "# YOLO目标检测训练数据加载器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f273ac00-2e3b-4fc6-902f-9d608b0f0c00",
   "metadata": {},
   "source": [
    "将训练/验证用的数据（图片路径 + 边框信息）转换成模型可以使用的格式，支持图像增强、归一化、标签转换等操作，并可以批量打包数据用于训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60d1cd87-4631-44a6-a01b-11822664f82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "import import_ipynb\n",
    "from utils import cvtColor, preprocess_input\n",
    "\n",
    "\n",
    "class YoloDataset(Dataset): \n",
    "    def __init__(self, annotation_lines, input_shape, num_classes, train):\n",
    "        super(YoloDataset, self).__init__()\n",
    "        self.annotation_lines   = annotation_lines   # annotation_lines: Each line follows the format image_path x1,y1,x2,y2,class_id ...\n",
    "        self.input_shape        = input_shape\n",
    "        self.num_classes        = num_classes\n",
    "        self.length             = len(self.annotation_lines)\n",
    "        self.train              = train  # train: Indicates whether it is in training mode, which determines whether data augmentation is applied.\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index): \n",
    "        index       = index % self.length \n",
    "        #---------------------------------------------------#\n",
    "        #   Random data augmentation is applied during training  \n",
    "        #   No random data augmentation is applied during validation\n",
    "        #---------------------------------------------------#\n",
    "        image, box  = self.get_random_data(self.annotation_lines[index], self.input_shape[0:2], random = self.train)\n",
    "        # image: The augmented image (already resized)  \n",
    "        # box: Bounding box information for each object, in the format [x1, y1, x2, y2, class_id]\n",
    "        image       = np.transpose(preprocess_input(np.array(image, dtype=np.float32)), (2, 0, 1))\n",
    "        box         = np.array(box, dtype=np.float32) \n",
    "        if len(box) != 0:\n",
    "            # Convert box format from [x1, y1, x2, y2] to [x_center, y_center, w, h], and normalize the values\n",
    "            box[:, [0, 2]] = box[:, [0, 2]] / self.input_shape[1]\n",
    "            box[:, [1, 3]] = box[:, [1, 3]] / self.input_shape[0] \n",
    "            # Normalize the bounding box coordinates to the range [0, 1] to facilitate network learning.\n",
    "            box[:, 2:4] = box[:, 2:4] - box[:, 0:2] # Calculate width and height: w = x2 - x1, h = y2 - y1\n",
    "            box[:, 0:2] = box[:, 0:2] + box[:, 2:4] / 2 # Calculate center coordinates: x_center = x1 + w / 2, y_center = y1 + h / 2\n",
    "        return image, box \n",
    "        # Return one image along with the information of multiple bounding boxes.\n",
    "\n",
    "    def rand(self, a=0, b=1):\n",
    "        return np.random.rand()*(b-a) + a \n",
    "\n",
    "    def get_random_data(self, annotation_line, input_shape, jitter=.3, hue=.1, sat=0.7, val=0.4, random=True): # 作用：从一行标注中读取图像，执行数据增强，返回图像数据和对应的目标框坐标。\n",
    "        line    = annotation_line.split()\n",
    "        #------------------------------#\n",
    "        # Read the image and convert it to an RGB image\n",
    "        #------------------------------#\n",
    "        image   = Image.open(line[0])\n",
    "        image   = cvtColor(image)\n",
    "        #------------------------------#\n",
    "        # Obtain the original image height and width, as well as the target height and width\n",
    "        #------------------------------#\n",
    "        iw, ih  = image.size\n",
    "        h, w    = input_shape\n",
    "        #------------------------------#\n",
    "        #   Obtain the predicted bounding boxes\n",
    "        #------------------------------#\n",
    "        box     = np.array([np.array(list(map(int,box.split(',')))) for box in line[1:]])\n",
    "\n",
    "        # When no augmentation is applied:\n",
    "        if not random:\n",
    "            scale = min(w/iw, h/ih)\n",
    "            nw = int(iw*scale)\n",
    "            nh = int(ih*scale)\n",
    "            dx = (w-nw)//2\n",
    "            dy = (h-nh)//2\n",
    "\n",
    "            #---------------------------------#\n",
    "            #   Pad the extra parts of the image with gray bars.\n",
    "            #---------------------------------#\n",
    "            image       = image.resize((nw,nh), Image.BICUBIC)\n",
    "            new_image   = Image.new('RGB', (w,h), (128,128,128))\n",
    "            new_image.paste(image, (dx, dy))\n",
    "            image_data  = np.array(new_image, np.float32)\n",
    "\n",
    "            #---------------------------------#\n",
    "            # Adjust the ground truth bounding boxes\n",
    "            #---------------------------------#\n",
    "            if len(box)>0:\n",
    "                np.random.shuffle(box)\n",
    "                box[:, [0,2]] = box[:, [0,2]]*nw/iw + dx\n",
    "                box[:, [1,3]] = box[:, [1,3]]*nh/ih + dy\n",
    "                box[:, 0:2][box[:, 0:2]<0] = 0\n",
    "                box[:, 2][box[:, 2]>w] = w\n",
    "                box[:, 3][box[:, 3]>h] = h\n",
    "                box_w = box[:, 2] - box[:, 0]\n",
    "                box_h = box[:, 3] - box[:, 1]\n",
    "                box = box[np.logical_and(box_w>1, box_h>1)] # discard invalid box\n",
    "\n",
    "            return image_data, box\n",
    "            \n",
    "        # If random augmentation is applied:      \n",
    "        #------------------------------------------#\n",
    "        #   Scale the image and apply aspect ratio distortions to its height and width\n",
    "        #------------------------------------------#\n",
    "        new_ar = iw/ih * self.rand(1-jitter,1+jitter) / self.rand(1-jitter,1+jitter)\n",
    "        scale = self.rand(.25, 2)\n",
    "        if new_ar < 1:\n",
    "            nh = int(scale*h)\n",
    "            nw = int(nh*new_ar)\n",
    "        else:\n",
    "            nw = int(scale*w)\n",
    "            nh = int(nw/new_ar)\n",
    "        image = image.resize((nw,nh), Image.BICUBIC)\n",
    "        \n",
    "        #------------------------------------------#\n",
    "        # Pad the extra areas of the image with gray bars\n",
    "        #------------------------------------------#\n",
    "        dx = int(self.rand(0, w-nw))\n",
    "        dy = int(self.rand(0, h-nh))\n",
    "        new_image = Image.new('RGB', (w,h), (128,128,128))\n",
    "        new_image.paste(image, (dx, dy))\n",
    "        image = new_image\n",
    "\n",
    "        #------------------------------------------#\n",
    "        #   Flip the image\n",
    "        #------------------------------------------#\n",
    "        flip = self.rand()<.5\n",
    "        if flip: image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "        image_data      = np.array(image, np.uint8)\n",
    "        \n",
    "        #---------------------------------#\n",
    "        #   Apply color space transformation to the image  \n",
    "        #   Calculate the parameters for the color space transformation\n",
    "        #---------------------------------#\n",
    "        r               = np.random.uniform(-1, 1, 3) * [hue, sat, val] + 1\n",
    "        #---------------------------------#\n",
    "        # Convert the image to HSV color space\n",
    "        #---------------------------------#\n",
    "        hue, sat, val   = cv2.split(cv2.cvtColor(image_data, cv2.COLOR_RGB2HSV))\n",
    "        dtype           = image_data.dtype\n",
    "        #---------------------------------#\n",
    "        # Apply the transformation\n",
    "        #---------------------------------#\n",
    "        x       = np.arange(0, 256, dtype=r.dtype)\n",
    "        lut_hue = ((x * r[0]) % 180).astype(dtype)\n",
    "        lut_sat = np.clip(x * r[1], 0, 255).astype(dtype)\n",
    "        lut_val = np.clip(x * r[2], 0, 255).astype(dtype)\n",
    "\n",
    "        image_data = cv2.merge((cv2.LUT(hue, lut_hue), cv2.LUT(sat, lut_sat), cv2.LUT(val, lut_val)))\n",
    "        image_data = cv2.cvtColor(image_data, cv2.COLOR_HSV2RGB)\n",
    "        \n",
    "        #---------------------------------#\n",
    "        # Adjust the ground truth bounding boxes\n",
    "        #---------------------------------#\n",
    "        if len(box)>0:\n",
    "            np.random.shuffle(box)\n",
    "            box[:, [0,2]] = box[:, [0,2]]*nw/iw + dx\n",
    "            box[:, [1,3]] = box[:, [1,3]]*nh/ih + dy\n",
    "            if flip: box[:, [0,2]] = w - box[:, [2,0]]\n",
    "            box[:, 0:2][box[:, 0:2]<0] = 0\n",
    "            box[:, 2][box[:, 2]>w] = w\n",
    "            box[:, 3][box[:, 3]>h] = h\n",
    "            box_w = box[:, 2] - box[:, 0]\n",
    "            box_h = box[:, 3] - box[:, 1]\n",
    "            box = box[np.logical_and(box_w>1, box_h>1)] \n",
    "        return image_data, box \n",
    "        # image_data: The augmented image, type is np.array\n",
    "        # box: The updated bounding boxes after augmentation, shape [N, 5], where the last dimension is the class ID\n",
    "\n",
    "def yolo_dataset_collate(batch):\n",
    "    images = []\n",
    "    bboxes = []\n",
    "    for img, box in batch:\n",
    "        images.append(img)\n",
    "        bboxes.append(box)\n",
    "    images = torch.from_numpy(np.array(images)).type(torch.FloatTensor)\n",
    "    bboxes = [torch.from_numpy(ann).type(torch.FloatTensor) for ann in bboxes]\n",
    "    return images, bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b14711c-5dc7-452c-8170-e6bbafc884cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch (GPU)",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
