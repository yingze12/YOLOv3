{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f9027a9-7801-4677-b799-79d7cc2a5381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: import-ipynb in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (0.2)\n",
      "Requirement already satisfied: IPython in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from import-ipynb) (8.30.0)\n",
      "Requirement already satisfied: nbformat in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from import-ipynb) (5.10.4)\n",
      "Requirement already satisfied: decorator in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (2.15.1)\n",
      "Requirement already satisfied: stack-data in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (5.14.3)\n",
      "Requirement already satisfied: exceptiongroup in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (4.12.2)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (0.4.6)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from nbformat->import-ipynb) (2.21.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from nbformat->import-ipynb) (4.23.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from nbformat->import-ipynb) (5.7.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from jedi>=0.16->IPython->import-ipynb) (0.8.4)\n",
      "Requirement already satisfied: attrs>=22.2.0 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from jsonschema>=2.6->nbformat->import-ipynb) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from jsonschema>=2.6->nbformat->import-ipynb) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from jsonschema>=2.6->nbformat->import-ipynb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from jsonschema>=2.6->nbformat->import-ipynb) (0.24.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->import-ipynb) (3.10.0)\n",
      "Requirement already satisfied: pywin32>=300 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->import-ipynb) (308)\n",
      "Requirement already satisfied: wcwidth in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->IPython->import-ipynb) (0.2.5)\n",
      "Requirement already satisfied: executing in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from stack-data->IPython->import-ipynb) (0.8.3)\n",
      "Requirement already satisfied: asttokens in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from stack-data->IPython->import-ipynb) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from stack-data->IPython->import-ipynb) (0.2.2)\n",
      "Requirement already satisfied: six in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from asttokens->stack-data->IPython->import-ipynb) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install import-ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d3e2258-47ff-4f17-aa1c-c9c25be0567b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: import-ipynb in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (0.2)\n",
      "Requirement already satisfied: IPython in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from import-ipynb) (8.30.0)\n",
      "Requirement already satisfied: nbformat in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from import-ipynb) (5.10.4)\n",
      "Requirement already satisfied: decorator in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (2.15.1)\n",
      "Requirement already satisfied: stack-data in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (5.14.3)\n",
      "Requirement already satisfied: exceptiongroup in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (4.12.2)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (0.4.6)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from nbformat->import-ipynb) (2.21.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from nbformat->import-ipynb) (4.23.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from nbformat->import-ipynb) (5.7.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from jedi>=0.16->IPython->import-ipynb) (0.8.4)\n",
      "Requirement already satisfied: attrs>=22.2.0 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from jsonschema>=2.6->nbformat->import-ipynb) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from jsonschema>=2.6->nbformat->import-ipynb) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from jsonschema>=2.6->nbformat->import-ipynb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from jsonschema>=2.6->nbformat->import-ipynb) (0.24.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->import-ipynb) (3.10.0)\n",
      "Requirement already satisfied: pywin32>=300 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->import-ipynb) (308)\n",
      "Requirement already satisfied: wcwidth in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->IPython->import-ipynb) (0.2.5)\n",
      "Requirement already satisfied: executing in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from stack-data->IPython->import-ipynb) (0.8.3)\n",
      "Requirement already satisfied: asttokens in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from stack-data->IPython->import-ipynb) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from stack-data->IPython->import-ipynb) (0.2.2)\n",
      "Requirement already satisfied: six in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from asttokens->stack-data->IPython->import-ipynb) (1.16.0)\n",
      "Requirement already satisfied: import-ipynb in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (0.2)\n",
      "Requirement already satisfied: IPython in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from import-ipynb) (8.30.0)\n",
      "Requirement already satisfied: nbformat in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from import-ipynb) (5.10.4)\n",
      "Requirement already satisfied: decorator in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (2.15.1)\n",
      "Requirement already satisfied: stack-data in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (5.14.3)\n",
      "Requirement already satisfied: exceptiongroup in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (4.12.2)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (0.4.6)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from nbformat->import-ipynb) (2.21.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from nbformat->import-ipynb) (4.23.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from nbformat->import-ipynb) (5.7.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from jedi>=0.16->IPython->import-ipynb) (0.8.4)\n",
      "Requirement already satisfied: attrs>=22.2.0 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from jsonschema>=2.6->nbformat->import-ipynb) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from jsonschema>=2.6->nbformat->import-ipynb) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from jsonschema>=2.6->nbformat->import-ipynb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from jsonschema>=2.6->nbformat->import-ipynb) (0.24.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->import-ipynb) (3.10.0)\n",
      "Requirement already satisfied: pywin32>=300 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->import-ipynb) (308)\n",
      "Requirement already satisfied: wcwidth in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->IPython->import-ipynb) (0.2.5)\n",
      "Requirement already satisfied: executing in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from stack-data->IPython->import-ipynb) (0.8.3)\n",
      "Requirement already satisfied: asttokens in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from stack-data->IPython->import-ipynb) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from stack-data->IPython->import-ipynb) (0.2.2)\n",
      "Requirement already satisfied: six in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from asttokens->stack-data->IPython->import-ipynb) (1.16.0)\n",
      "initialize network with normal type\n",
      "Load weights model_data/yolo_weights.pth.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yingz\\AppData\\Local\\Temp\\ipykernel_22332\\64064003.py:189: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_dict = torch.load(model_path, map_location = device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successful Load Key: ['backbone.conv1.weight', 'backbone.bn1.weight', 'backbone.bn1.bias', 'backbone.bn1.running_mean', 'backbone.bn1.running_var', 'backbone.layer1.ds_conv.weight', 'backbone.layer1.ds_bn.weight', 'backbone.layer1.ds_bn.bias', 'backbone.layer1.ds_bn.running_mean', 'backbone.layer1.ds_bn.running_var', 'backbone.layer1.residual_0.conv1.weight', 'backbone.layer1.residual_0.bn1.weight', 'backbone.layer1.residual_0.bn1.bias', 'backbone.layer1.residual_0.bn1.running_mean', 'backbone.layer1.residual_0.bn1. ……\n",
      "Successful Load Key Num: 360\n",
      "\n",
      "Fail To Load Key: ['last_layer0.6.weight', 'last_layer0.6.bias', 'last_layer1.6.weight', 'last_layer1.6.bias', 'last_layer2.6.weight', 'last_layer2.6.bias'] ……\n",
      "Fail To Load Key num: 6\n",
      "\n",
      "\u001b[1;33;44mFriendly reminder: It is normal for the head part not to be loaded; however, it is an error if the backbone part is not loaded.\u001b[0m\n",
      "Configurations:\n",
      "----------------------------------------------------------------------\n",
      "|                     keys |                                   values|\n",
      "----------------------------------------------------------------------\n",
      "|             classes_path |               model_data/voc_classes.txt|\n",
      "|             anchors_path |              model_data/yolo_anchors.txt|\n",
      "|             anchors_mask |        [[6, 7, 8], [3, 4, 5], [0, 1, 2]]|\n",
      "|               model_path |              model_data/yolo_weights.pth|\n",
      "|              input_shape |                               [416, 416]|\n",
      "|               Init_Epoch |                                        0|\n",
      "|             Freeze_Epoch |                                       50|\n",
      "|           UnFreeze_Epoch |                                      300|\n",
      "|        Freeze_batch_size |                                       16|\n",
      "|      Unfreeze_batch_size |                                        8|\n",
      "|             Freeze_Train |                                     True|\n",
      "|                  Init_lr |                                     0.01|\n",
      "|                   Min_lr |                                   0.0001|\n",
      "|           optimizer_type |                                      sgd|\n",
      "|                 momentum |                                    0.937|\n",
      "|            lr_decay_type |                                      cos|\n",
      "|              save_period |                                       10|\n",
      "|                 save_dir |                                     logs|\n",
      "|              num_workers |                                        0|\n",
      "|                num_train |                                     4008|\n",
      "|                  num_val |                                     1003|\n",
      "----------------------------------------------------------------------\n",
      "Start Train\n",
      "Epoch 1/300:   6%|██▉                                          | 16/250 [00:12<02:13,  1.75it/s, loss=7.21, lr=0.00025]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 382\u001b[0m\n\u001b[0;32m    379\u001b[0m     train_sampler\u001b[38;5;241m.\u001b[39mset_epoch(epoch)\n\u001b[0;32m    380\u001b[0m set_optimizer_lr(optimizer, lr_scheduler_func, epoch)\n\u001b[1;32m--> 382\u001b[0m \u001b[43mfit_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myolo_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_history\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_step_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mUnFreeze_Epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCuda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_period\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_rank\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m distributed:\n\u001b[0;32m    385\u001b[0m     dist\u001b[38;5;241m.\u001b[39mbarrier()\n",
      "File \u001b[1;32m<string>:76\u001b[0m, in \u001b[0;36mfit_one_epoch\u001b[1;34m(model_train, model, yolo_loss, loss_history, eval_callback, optimizer, epoch, epoch_step, epoch_step_val, gen, gen_val, Epoch, cuda, fp16, scaler, save_period, save_dir, local_rank)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the dataset\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import import_ipynb\n",
    "from darknet import YoloBody\n",
    "from yolo_training import (YOLOLoss, get_lr_scheduler, set_optimizer_lr, weights_init)\n",
    "from callbacks import EvalCallback, LossHistory\n",
    "from dataloader import YoloDataset, yolo_dataset_collate\n",
    "from utils import (get_anchors, get_classes, seed_everything, show_config, worker_init_fn)\n",
    "from utils_fit import fit_one_epoch\n",
    "\n",
    "'''\n",
    "When training your own object detection model, make sure to pay attention to the following key points:\n",
    "Before training, carefully check whether your dataset format meets the requirements.\n",
    "This library requires the dataset to be in VOC format, which means you need to prepare:\n",
    "\n",
    "Input images: These should be .jpg files. The size does not need to be fixed, as resizing will be handled automatically before training.\n",
    "Grayscale images will be automatically converted to RGB, so you don’t need to convert them manually.\n",
    "If the image extension is not .jpg, you must batch convert them to .jpg before starting training.\n",
    "\n",
    "Labels: These should be .xml files containing the target information to be detected.\n",
    "Each label file should correspond to one input image file.\n",
    "\n",
    "The trained weight files are saved in the logs folder.\n",
    "Each training epoch consists of several training steps (Steps), and one gradient descent update is performed per step.\n",
    "If you only train for a few steps, the weights will not be saved.\n",
    "'''\n",
    "if __name__ == \"__main__\":\n",
    "    Cuda            = True\n",
    "    # Seed — Used to set a fixed random seed  \n",
    "    #         Ensures that each independent training run produces the same results\n",
    "    seed            = 11\n",
    "    distributed     = False\n",
    "    sync_bn         = False\n",
    "    fp16            = False\n",
    "    classes_path    = 'model_data/voc_classes.txt'\n",
    "    anchors_path    = 'model_data/yolo_anchors.txt'\n",
    "    anchors_mask    = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]\n",
    "    # The most important part of the model's pre-trained weights is the backbone feature extraction network,  \n",
    "    # which is used for extracting features.  \n",
    "    # Here, the weights of the entire model are used, so they are loaded in `train.py`.  \n",
    "    # The `pretrain` setting below does not affect the loading of these weights.\n",
    "    model_path      = 'model_data/yolo_weights.pth'\n",
    "    # input_shape — The input shape size, must be a multiple of 32\n",
    "    input_shape     = [416, 416]\n",
    "    # pretrained — If model_path is set, the backbone weights do not need to be loaded separately, so the value of pretrained becomes meaningless.\n",
    "    pretrained      = False\n",
    "    #----------------------------------------------------------------------------------------------------------------------------#\n",
    "    # The training is divided into two phases: the frozen phase and the unfrozen phase.  \n",
    "    # Frozen training requires less GPU memory. If your GPU is very weak, you can set `Freeze_Epoch` equal to `UnFreeze_Epoch`,  \n",
    "    # which means only frozen training will be performed.\n",
    "    \n",
    "    # Here are some suggested parameter settings. You can flexibly adjust them according to your own needs:\n",
    "    # (1) Training from the pre-trained weights of the whole model:\n",
    "    #     Adam optimizer:\n",
    "    #         Init_Epoch = 0, Freeze_Epoch = 50, UnFreeze_Epoch = 100, Freeze_Train = True,\n",
    "    #         optimizer_type = 'adam', Init_lr = 1e-3, weight_decay = 0.  (Frozen)\n",
    "    #         Init_Epoch = 0, UnFreeze_Epoch = 100, Freeze_Train = False,\n",
    "    #         optimizer_type = 'adam', Init_lr = 1e-3, weight_decay = 0.  (Unfrozen)\n",
    "    #     SGD optimizer:\n",
    "    #         Init_Epoch = 0, Freeze_Epoch = 50, UnFreeze_Epoch = 300, Freeze_Train = True,\n",
    "    #         optimizer_type = 'sgd', Init_lr = 1e-2, weight_decay = 5e-4.  (Frozen)\n",
    "    #         Init_Epoch = 0, UnFreeze_Epoch = 300, Freeze_Train = False,\n",
    "    #         optimizer_type = 'sgd', Init_lr = 1e-2, weight_decay = 5e-4.  (Unfrozen)\n",
    "    #         Note: UnFreeze_Epoch can be adjusted between 100 and 300.\n",
    "    # (2) Setting `batch_size`:\n",
    "    #     Use the largest possible value that your GPU can handle.  \n",
    "    #     Insufficient GPU memory (errors like OOM or CUDA out of memory) is unrelated to dataset size — reduce `batch_size` if this occurs.  \n",
    "    #     Due to the influence of BatchNorm layers, the minimum `batch_size` is 2 and cannot be 1.  \n",
    "    #     Normally, it is recommended that `Freeze_batch_size` be 1–2 times the `Unfreeze_batch_size`.  \n",
    "    #     Avoid setting them too far apart, as it affects automatic learning rate adjustment.\n",
    "    #----------------------------------------------------------------------------------------------------------------------------#\n",
    "    #------------------------------------------------------------------#\n",
    "    # Frozen phase training parameters  \n",
    "    # During this phase, the model's backbone is frozen, meaning the feature extraction network does not update.  \n",
    "    # This phase uses less GPU memory and only fine-tunes the rest of the network.\n",
    "    \n",
    "    # Init_Epoch — The starting epoch of the model training. It can be set higher than `Freeze_Epoch`, e.g.:  \n",
    "    #              Init_Epoch = 60, Freeze_Epoch = 50, UnFreeze_Epoch = 100  \n",
    "    #              This skips the frozen phase and starts directly from epoch 60 with the adjusted learning rate.  \n",
    "    #              (Used for resuming training from a checkpoint)\n",
    "    \n",
    "    # Freeze_Epoch — The number of epochs for the frozen training phase  \n",
    "    #                (This setting is ignored if `Freeze_Train=False`)\n",
    "    # Freeze_batch_size — The batch size used during frozen training  \n",
    "    #                (This setting is ignored if `Freeze_Train=False`)\n",
    "    #------------------------------------------------------------------#\n",
    "    Init_Epoch          = 0\n",
    "    Freeze_Epoch        = 50\n",
    "    Freeze_batch_size   = 16\n",
    "    #------------------------------------------------------------------#\n",
    "    # Unfrozen phase training parameters  \n",
    "    # During this phase, the model's backbone is no longer frozen, so the feature extraction network will be updated.  \n",
    "    # This phase consumes more GPU memory, and all network parameters are trained.\n",
    "    \n",
    "    # UnFreeze_Epoch — Total number of epochs for the entire training process  \n",
    "    #                  SGD usually requires a longer convergence time, so a larger `UnFreeze_Epoch` is recommended.  \n",
    "    #                  Adam optimizer can use a relatively smaller `UnFreeze_Epoch`.\n",
    "    \n",
    "    # Unfreeze_batch_size — The batch size used during the unfrozen phase\n",
    "    #------------------------------------------------------------------#\n",
    "    UnFreeze_Epoch      = 300\n",
    "    Unfreeze_batch_size = 8\n",
    "\n",
    "    Freeze_Train        = True\n",
    "\n",
    "    # Other training parameters: learning rate, optimizer, and learning rate decay settings\n",
    "\n",
    "    # Init_lr — The maximum learning rate of the model  \n",
    "    # Min_lr  — The minimum learning rate of the model, defaulting to 1% of the maximum learning rate\n",
    "    Init_lr             = 1e-2\n",
    "    Min_lr              = Init_lr * 0.01\n",
    "    #------------------------------------------------------------------#\n",
    "    # optimizer_type — Type of optimizer to use; options are 'adam' and 'sgd'  \n",
    "    #                  Recommended settings:  \n",
    "    #                  - For Adam optimizer: Init_lr = 1e-3  \n",
    "    #                  - For SGD optimizer:  Init_lr = 1e-2\n",
    "    # momentum — Momentum parameter used internally by the optimizer  \n",
    "    # weight_decay — Weight decay used to prevent overfitting  \n",
    "    # Adam may cause issues with weight_decay, so it is recommended to set it to 0 when using Adam.\n",
    "    #------------------------------------------------------------------#\n",
    "    optimizer_type      = \"sgd\"\n",
    "    momentum            = 0.937\n",
    "    weight_decay        = 5e-4\n",
    "    # # lr_decay_type — Type of learning rate decay; options are 'step' and 'cos'\n",
    "    lr_decay_type       = \"cos\"\n",
    "    # save_period — Save the model weights every specified number of epochs\n",
    "    save_period         = 10\n",
    "    # save_dir — Folder where the model weights and log files will be saved\n",
    "    save_dir            = 'logs'\n",
    "    #------------------------------------------------------------------#\n",
    "    # eval_flag — Whether to perform evaluation during training; the evaluation is done on the validation set  \n",
    "    # eval_period — Number of epochs between each evaluation\n",
    "    #------------------------------------------------------------------#\n",
    "    eval_flag           = True\n",
    "    eval_period         = 10\n",
    "    # num_workers — Sets whether to use multi-threading for data loading  \n",
    "    # Enabling this speeds up data loading but uses more memory  \n",
    "    # For computers with limited memory, it can be set to 2 or 0\n",
    "    num_workers         = 0\n",
    "\n",
    "    # Get image paths and labels\n",
    "    train_annotation_path   = '2007_train.txt'\n",
    "    val_annotation_path     = '2007_val.txt'\n",
    "\n",
    "    seed_everything(seed)\n",
    "    #   Set the GPU to be used\n",
    "    ngpus_per_node  = torch.cuda.device_count()\n",
    "    if distributed:\n",
    "        dist.init_process_group(backend=\"nccl\")\n",
    "        local_rank  = int(os.environ[\"LOCAL_RANK\"])\n",
    "        rank        = int(os.environ[\"RANK\"])\n",
    "        device      = torch.device(\"cuda\", local_rank)\n",
    "        if local_rank == 0:\n",
    "            print(f\"[{os.getpid()}] (rank = {rank}, local_rank = {local_rank}) training...\")\n",
    "            print(\"Gpu Device Count : \", ngpus_per_node)\n",
    "    else:\n",
    "        device          = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        local_rank      = 0\n",
    "        rank            = 0\n",
    "\n",
    "    #   Get classes and anchors\n",
    "    class_names, num_classes = get_classes(classes_path)\n",
    "    anchors, num_anchors     = get_anchors(anchors_path)\n",
    "        \n",
    "    # Create the YOLO model\n",
    "    model = YoloBody(anchors_mask, num_classes, pretrained=pretrained)\n",
    "    if not pretrained:\n",
    "        weights_init(model)\n",
    "    if model_path != '':\n",
    "        if local_rank == 0:\n",
    "            print('Load weights {}.'.format(model_path))\n",
    "        \n",
    "        #   Load according to the keys of the pre-trained weights and the model's keys.\n",
    "        model_dict      = model.state_dict()\n",
    "        pretrained_dict = torch.load(model_path, map_location = device)\n",
    "        load_key, no_load_key, temp_dict = [], [], {}\n",
    "        for k, v in pretrained_dict.items():\n",
    "            if k in model_dict.keys() and np.shape(model_dict[k]) == np.shape(v):\n",
    "                temp_dict[k] = v\n",
    "                load_key.append(k)\n",
    "            else:\n",
    "                no_load_key.append(k)\n",
    "        model_dict.update(temp_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "        #   Display the unmatched keys.\n",
    "        if local_rank == 0:\n",
    "            print(\"\\nSuccessful Load Key:\", str(load_key)[:500], \"……\\nSuccessful Load Key Num:\", len(load_key))\n",
    "            print(\"\\nFail To Load Key:\", str(no_load_key)[:500], \"……\\nFail To Load Key num:\", len(no_load_key))\n",
    "            print(\"\\n\\033[1;33;44mFriendly reminder: It is normal for the head part not to be loaded; however, it is an error if the backbone part is not loaded.\\033[0m\")\n",
    "\n",
    "    # Get the loss function\n",
    "    yolo_loss = YOLOLoss(anchors, num_classes, input_shape, Cuda, anchors_mask)\n",
    "    #   Record the loss\n",
    "    if local_rank == 0:\n",
    "        time_str        = datetime.datetime.strftime(datetime.datetime.now(),'%Y_%m_%d_%H_%M_%S')\n",
    "        log_dir         = os.path.join(save_dir, \"loss_\" + str(time_str))\n",
    "        loss_history    = LossHistory(log_dir, model, input_shape=input_shape)\n",
    "    else:\n",
    "        loss_history    = None\n",
    "        \n",
    "    if fp16:\n",
    "        from torch.cuda.amp import GradScaler as GradScaler\n",
    "        scaler = GradScaler()\n",
    "    else:\n",
    "        scaler = None\n",
    "\n",
    "    model_train     = model.train()\n",
    "    #   Synchronize BatchNorm across multiple GPUs\n",
    "    if sync_bn and ngpus_per_node > 1 and distributed:\n",
    "        model_train = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model_train)\n",
    "    elif sync_bn:\n",
    "        print(\"Sync_bn is not support in one gpu or not distributed.\")\n",
    "\n",
    "    if Cuda:\n",
    "        if distributed:\n",
    "            #----------------------------#\n",
    "            #   Parallel execution on multiple GPUs\n",
    "            #----------------------------#\n",
    "            model_train = model_train.cuda(local_rank)\n",
    "            model_train = torch.nn.parallel.DistributedDataParallel(model_train, device_ids=[local_rank], find_unused_parameters=True)\n",
    "        else:\n",
    "            model_train = torch.nn.DataParallel(model)\n",
    "            cudnn.benchmark = True\n",
    "            model_train = model_train.cuda()\n",
    "\n",
    "    #   Read the txt file corresponding to the dataset\n",
    "    with open(train_annotation_path) as f:\n",
    "        train_lines = f.readlines()\n",
    "    with open(val_annotation_path) as f:\n",
    "        val_lines   = f.readlines()\n",
    "    num_train   = len(train_lines)\n",
    "    num_val     = len(val_lines)\n",
    "\n",
    "    if local_rank == 0:\n",
    "        show_config(\n",
    "            classes_path = classes_path, anchors_path = anchors_path, anchors_mask = anchors_mask, model_path = model_path, input_shape = input_shape, \\\n",
    "            Init_Epoch = Init_Epoch, Freeze_Epoch = Freeze_Epoch, UnFreeze_Epoch = UnFreeze_Epoch, Freeze_batch_size = Freeze_batch_size, Unfreeze_batch_size = Unfreeze_batch_size, Freeze_Train = Freeze_Train, \\\n",
    "            Init_lr = Init_lr, Min_lr = Min_lr, optimizer_type = optimizer_type, momentum = momentum, lr_decay_type = lr_decay_type, \\\n",
    "            save_period = save_period, save_dir = save_dir, num_workers = num_workers, num_train = num_train, num_val = num_val\n",
    "        )\n",
    "        \n",
    "        wanted_step = 5e4 if optimizer_type == \"sgd\" else 1.5e4\n",
    "        total_step  = num_train // Unfreeze_batch_size * UnFreeze_Epoch\n",
    "        if total_step <= wanted_step:\n",
    "            if num_train // Unfreeze_batch_size == 0:\n",
    "                raise ValueError('The dataset is too small to train the model. Please expand the dataset.')\n",
    "            wanted_epoch = wanted_step // (num_train // Unfreeze_batch_size) + 1\n",
    "            print(\"\\n\\033[1;33;44m[Warning] When using the %s optimizer, it is recommended to set the total training steps to at least %d.\\033[0m\" % (optimizer_type, wanted_step))\n",
    "            print(\"\\033[1;33;44m[Warning] This run has a total of %d training samples, Unfreeze_batch_size is %d, training for %d epochs, resulting in %d total training steps.\\033[0m\" % (num_train, Unfreeze_batch_size, UnFreeze_Epoch, total_step))\n",
    "            print(\"\\033[1;33;44m[Warning] Since the total training steps is %d, which is less than the recommended %d, it is advised to set the total epochs to %d.\\033[0m\" %())\n",
    "\n",
    "    if True:\n",
    "        UnFreeze_flag = False\n",
    "        #   Freeze a certain part of the model for training\n",
    "        if Freeze_Train:\n",
    "            for param in model.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # If not performing frozen training, directly set batch_size to Unfreeze_batch_size\n",
    "        batch_size = Freeze_batch_size if Freeze_Train else Unfreeze_batch_size\n",
    "\n",
    "        # Determine the current batch_size and adaptively adjust the learning rate\n",
    "        nbs             = 64\n",
    "        lr_limit_max    = 1e-3 if optimizer_type == 'adam' else 5e-2\n",
    "        lr_limit_min    = 3e-4 if optimizer_type == 'adam' else 5e-4\n",
    "        Init_lr_fit     = min(max(batch_size / nbs * Init_lr, lr_limit_min), lr_limit_max)\n",
    "        Min_lr_fit      = min(max(batch_size / nbs * Min_lr, lr_limit_min * 1e-2), lr_limit_max * 1e-2)\n",
    "\n",
    "        #   Select the optimizer based on optimizer_type\n",
    "        pg0, pg1, pg2 = [], [], []  \n",
    "        for k, v in model.named_modules():\n",
    "            if hasattr(v, \"bias\") and isinstance(v.bias, nn.Parameter):\n",
    "                pg2.append(v.bias)    \n",
    "            if isinstance(v, nn.BatchNorm2d) or \"bn\" in k:\n",
    "                pg0.append(v.weight)    \n",
    "            elif hasattr(v, \"weight\") and isinstance(v.weight, nn.Parameter):\n",
    "                pg1.append(v.weight)   \n",
    "        optimizer = {\n",
    "            'adam'  : optim.Adam(pg0, Init_lr_fit, betas = (momentum, 0.999)),\n",
    "            'sgd'   : optim.SGD(pg0, Init_lr_fit, momentum = momentum, nesterov=True)\n",
    "        }[optimizer_type]\n",
    "        optimizer.add_param_group({\"params\": pg1, \"weight_decay\": weight_decay})\n",
    "        optimizer.add_param_group({\"params\": pg2})\n",
    "\n",
    "        #   Get the learning rate decay formula\n",
    "        lr_scheduler_func = get_lr_scheduler(lr_decay_type, Init_lr_fit, Min_lr_fit, UnFreeze_Epoch)\n",
    "        \n",
    "        #   Determine the length of each epoch\n",
    "        epoch_step      = num_train // batch_size\n",
    "        epoch_step_val  = num_val // batch_size\n",
    "        \n",
    "        if epoch_step == 0 or epoch_step_val == 0:\n",
    "            raise ValueError(\"The dataset is too small to continue training. Please expand the dataset.\")\n",
    "\n",
    "        # Build the dataset loaders.\n",
    "        train_dataset   = YoloDataset(train_lines, input_shape, num_classes, train = True)\n",
    "        val_dataset     = YoloDataset(val_lines, input_shape, num_classes, train = False)\n",
    "        \n",
    "        if distributed:\n",
    "            train_sampler   = torch.utils.data.distributed.DistributedSampler(train_dataset, shuffle=True,)\n",
    "            val_sampler     = torch.utils.data.distributed.DistributedSampler(val_dataset, shuffle=False,)\n",
    "            batch_size      = batch_size // ngpus_per_node\n",
    "            shuffle         = False\n",
    "        else:\n",
    "            train_sampler   = None\n",
    "            val_sampler     = None\n",
    "            shuffle         = True\n",
    "\n",
    "        gen             = DataLoader(train_dataset, shuffle = shuffle, batch_size = batch_size, num_workers = num_workers, pin_memory=True,\n",
    "                                    drop_last=True, collate_fn=yolo_dataset_collate, sampler=train_sampler, \n",
    "                                    worker_init_fn=partial(worker_init_fn, rank=rank, seed=seed))\n",
    "        gen_val         = DataLoader(val_dataset  , shuffle = shuffle, batch_size = batch_size, num_workers = num_workers, pin_memory=True, \n",
    "                                    drop_last=True, collate_fn=yolo_dataset_collate, sampler=val_sampler, \n",
    "                                    worker_init_fn=partial(worker_init_fn, rank=rank, seed=seed))\n",
    "\n",
    "        # Record the mAP curve during evaluation\n",
    "        if local_rank == 0:\n",
    "            eval_callback   = EvalCallback(model, input_shape, anchors, anchors_mask, class_names, num_classes, val_lines, log_dir, Cuda, \\\n",
    "                                            eval_flag=eval_flag, period=eval_period)\n",
    "        else:\n",
    "            eval_callback   = None\n",
    "        \n",
    "        # Start model training\n",
    "        for epoch in range(Init_Epoch, UnFreeze_Epoch):\n",
    "            # If the model has a frozen part for learning  \n",
    "            # Then unfreeze it and set the corresponding parameters\n",
    "            if epoch >= Freeze_Epoch and not UnFreeze_flag and Freeze_Train: # Three conditions to enter the unfreezing logic:\n",
    "                batch_size = Unfreeze_batch_size\n",
    "                # Current epoch is greater than or equal to Freeze_Epoch;\n",
    "                # Unfreezing has not yet been performed (UnFreeze_flag == False);\n",
    "                # The user has set Freeze_Train = True.\n",
    "                #-------------------------------------------------------------------#\n",
    "                # Determine the current batch_size and adaptively adjust the learning rate\n",
    "                #-------------------------------------------------------------------#\n",
    "                nbs             = 64\n",
    "                lr_limit_max    = 1e-3 if optimizer_type == 'adam' else 5e-2\n",
    "                lr_limit_min    = 3e-4 if optimizer_type == 'adam' else 5e-4\n",
    "                Init_lr_fit     = min(max(batch_size / nbs * Init_lr, lr_limit_min), lr_limit_max)\n",
    "                Min_lr_fit      = min(max(batch_size / nbs * Min_lr, lr_limit_min * 1e-2), lr_limit_max * 1e-2)\n",
    "                # Get the learning rate decay formula\n",
    "                lr_scheduler_func = get_lr_scheduler(lr_decay_type, Init_lr_fit, Min_lr_fit, UnFreeze_Epoch)\n",
    "                \n",
    "                for param in model.backbone.parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "                epoch_step      = num_train // batch_size\n",
    "                epoch_step_val  = num_val // batch_size\n",
    "\n",
    "                if epoch_step == 0 or epoch_step_val == 0:\n",
    "                    raise ValueError(\"The dataset is too small to continue training. Please expand the dataset.\")\n",
    "\n",
    "                if distributed:\n",
    "                    batch_size = batch_size // ngpus_per_node\n",
    "                    \n",
    "                gen             = DataLoader(train_dataset, shuffle = shuffle, batch_size = batch_size, num_workers = num_workers, pin_memory=True,\n",
    "                                            drop_last=True, collate_fn=yolo_dataset_collate, sampler=train_sampler, \n",
    "                                            worker_init_fn=partial(worker_init_fn, rank=rank, seed=seed))\n",
    "                gen_val         = DataLoader(val_dataset  , shuffle = shuffle, batch_size = batch_size, num_workers = num_workers, pin_memory=True, \n",
    "                                            drop_last=True, collate_fn=yolo_dataset_collate, sampler=val_sampler, \n",
    "                                            worker_init_fn=partial(worker_init_fn, rank=rank, seed=seed))\n",
    "\n",
    "                UnFreeze_flag = True\n",
    "                \n",
    "            if distributed:\n",
    "                train_sampler.set_epoch(epoch)\n",
    "            set_optimizer_lr(optimizer, lr_scheduler_func, epoch)\n",
    "\n",
    "            fit_one_epoch(model_train, model, yolo_loss, loss_history, eval_callback, optimizer, epoch, epoch_step, epoch_step_val, gen, gen_val, UnFreeze_Epoch, Cuda, fp16, scaler, save_period, save_dir, local_rank)\n",
    "                        \n",
    "            if distributed:\n",
    "                dist.barrier()\n",
    "\n",
    "        if local_rank == 0:\n",
    "            loss_history.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedb9923-9e04-405f-bccb-2dc32c2a6406",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch (GPU)",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
