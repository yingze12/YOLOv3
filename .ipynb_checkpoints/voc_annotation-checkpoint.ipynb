{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf5d9187-94b0-47f9-aee7-7b6a887d8d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: import-ipynb in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (0.2)\n",
      "Requirement already satisfied: IPython in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from import-ipynb) (8.30.0)\n",
      "Requirement already satisfied: nbformat in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from import-ipynb) (5.10.4)\n",
      "Requirement already satisfied: decorator in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (2.15.1)\n",
      "Requirement already satisfied: stack-data in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (5.14.3)\n",
      "Requirement already satisfied: exceptiongroup in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (4.12.2)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (0.4.6)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from nbformat->import-ipynb) (2.21.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from nbformat->import-ipynb) (4.23.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from nbformat->import-ipynb) (5.7.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from jedi>=0.16->IPython->import-ipynb) (0.8.4)\n",
      "Requirement already satisfied: attrs>=22.2.0 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from jsonschema>=2.6->nbformat->import-ipynb) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from jsonschema>=2.6->nbformat->import-ipynb) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from jsonschema>=2.6->nbformat->import-ipynb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from jsonschema>=2.6->nbformat->import-ipynb) (0.24.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->import-ipynb) (3.10.0)\n",
      "Requirement already satisfied: pywin32>=300 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->import-ipynb) (308)\n",
      "Requirement already satisfied: wcwidth in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->IPython->import-ipynb) (0.2.5)\n",
      "Requirement already satisfied: executing in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from stack-data->IPython->import-ipynb) (0.8.3)\n",
      "Requirement already satisfied: asttokens in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from stack-data->IPython->import-ipynb) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from stack-data->IPython->import-ipynb) (0.2.2)\n",
      "Requirement already satisfied: six in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from asttokens->stack-data->IPython->import-ipynb) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install import-ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a95830a-7638-4415-a95b-c02c3bf74cfa",
   "metadata": {},
   "source": [
    "# YOLO模型训练数据准备阶段"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6891df44-8289-4777-81e4-55276502ad53",
   "metadata": {},
   "source": [
    "把VOC数据集的XML格式标签文件，转换成YOLO训练所需的文本文件,（如2007_train.txt和2007_val.txt），其中包含每张图片的路径，以及图片中每个目标（物体）的具体位置（坐标）和类别信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07bd0692-b43b-4379-b68c-d189b994dbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate 2007_train.txt and 2007_val.txt for train.\n",
      "Generate 2007_train.txt and 2007_val.txt for train done.\n",
      "|   aeroplane |  306 | \n",
      "|     bicycle |  353 | \n",
      "|        bird |  486 | \n",
      "|        boat |  290 | \n",
      "|      bottle |  505 | \n",
      "|         bus |  229 | \n",
      "|         car | 1250 | \n",
      "|         cat |  376 | \n",
      "|       chair |  798 | \n",
      "|         cow |  259 | \n",
      "| diningtable |  215 | \n",
      "|         dog |  510 | \n",
      "|       horse |  362 | \n",
      "|   motorbike |  339 | \n",
      "|      person | 4690 | \n",
      "| pottedplant |  514 | \n",
      "|       sheep |  257 | \n",
      "|        sofa |  248 | \n",
      "|       train |  297 | \n",
      "|   tvmonitor |  324 | \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import import_ipynb\n",
    "from utils import get_classes\n",
    "\n",
    "# annotation_mode set to 2 means generating the training files 2007_train.txt and 2007_val.txt.\n",
    "annotation_mode     = 2\n",
    "\n",
    "classes_path        = 'model_data/voc_classes.txt'\n",
    "\n",
    "# Points to the folder containing the VOC dataset\n",
    "# By default, it points to the VOC dataset located in the root directory\n",
    "VOCdevkit_path  = 'VOCdevkit'\n",
    "\n",
    "VOCdevkit_sets  = [('2007', 'train'), ('2007', 'val')]\n",
    "classes, _      = get_classes(classes_path)\n",
    "\n",
    "\n",
    "# Count the number of targets\n",
    "photo_nums  = np.zeros(len(VOCdevkit_sets))\n",
    "nums        = np.zeros(len(classes))\n",
    "def convert_annotation(year, image_id, list_file):\n",
    "    try:\n",
    "        in_file = open(os.path.join(VOCdevkit_path, 'VOC%s/Annotations/%s.xml' % (year, image_id)), encoding='utf-8')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: {image_id}.xml not found, skipping...\")\n",
    "        return  # Skip this image directly\n",
    "    \n",
    "    tree=ET.parse(in_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    for obj in root.iter('object'):\n",
    "        difficult = 0 \n",
    "        if obj.find('difficult')!=None:\n",
    "            difficult = obj.find('difficult').text\n",
    "        cls = obj.find('name').text\n",
    "        if cls not in classes or int(difficult)==1:\n",
    "            continue\n",
    "        cls_id = classes.index(cls)\n",
    "        xmlbox = obj.find('bndbox')\n",
    "        b = (int(float(xmlbox.find('xmin').text)), int(float(xmlbox.find('ymin').text)), int(float(xmlbox.find('xmax').text)), int(float(xmlbox.find('ymax').text)))\n",
    "        list_file.write(\" \" + \",\".join([str(a) for a in b]) + ',' + str(cls_id))\n",
    "        \n",
    "        nums[classes.index(cls)] = nums[classes.index(cls)] + 1\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    random.seed(0)\n",
    "    if \" \" in os.path.abspath(VOCdevkit_path):\n",
    "        raise ValueError(\"The dataset folder path and image names must not contain spaces; otherwise, normal model training will be affected. Please modify accordingly.\")\n",
    "\n",
    "    if annotation_mode == 2:\n",
    "        print(\"Generate 2007_train.txt and 2007_val.txt for train.\")\n",
    "        type_index = 0\n",
    "        for year, image_set in VOCdevkit_sets:\n",
    "            image_ids = open(os.path.join(VOCdevkit_path, 'VOC%s/ImageSets/Main/%s.txt'%(year, image_set)), encoding='utf-8').read().strip().split()\n",
    "            list_file = open('%s_%s.txt'%(year, image_set), 'w', encoding='utf-8')\n",
    "            for image_id in image_ids:\n",
    "                list_file.write('%s/VOC%s/JPEGImages/%s.jpg'%(os.path.abspath(VOCdevkit_path), year, image_id))\n",
    "\n",
    "                convert_annotation(year, image_id, list_file)\n",
    "                list_file.write('\\n')\n",
    "            photo_nums[type_index] = len(image_ids)\n",
    "            type_index += 1\n",
    "            list_file.close()\n",
    "        print(\"Generate 2007_train.txt and 2007_val.txt for train done.\")\n",
    "        \n",
    "        def printTable(List1, List2):\n",
    "            for i in range(len(List1[0])):\n",
    "                print(\"|\", end=' ')\n",
    "                for j in range(len(List1)):\n",
    "                    print(List1[j][i].rjust(int(List2[j])), end=' ')\n",
    "                    print(\"|\", end=' ')\n",
    "                print()\n",
    "\n",
    "        str_nums = [str(int(x)) for x in nums]\n",
    "        tableData = [\n",
    "            classes, str_nums\n",
    "        ]\n",
    "        colWidths = [0]*len(tableData)\n",
    "        len1 = 0\n",
    "        for i in range(len(tableData)):\n",
    "            for j in range(len(tableData[i])):\n",
    "                if len(tableData[i][j]) > colWidths[i]:\n",
    "                    colWidths[i] = len(tableData[i][j])\n",
    "        printTable(tableData, colWidths)\n",
    "\n",
    "        if photo_nums[0] <= 500:\n",
    "            print(\"The training set has fewer than 500 samples, which is considered a small dataset. Please ensure you set a higher number of training epochs to achieve sufficient gradient descent steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f27c397-c352-4e75-b889-da870a6787ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch (GPU)",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
