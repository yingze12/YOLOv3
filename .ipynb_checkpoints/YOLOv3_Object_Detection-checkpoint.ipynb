{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f3bb8c4-c73d-4ec6-ad85-f13dc6556634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: import-ipynb in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (0.2)\n",
      "Requirement already satisfied: IPython in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from import-ipynb) (8.30.0)\n",
      "Requirement already satisfied: nbformat in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from import-ipynb) (5.10.4)\n",
      "Requirement already satisfied: decorator in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (2.15.1)\n",
      "Requirement already satisfied: stack-data in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (5.14.3)\n",
      "Requirement already satisfied: exceptiongroup in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (4.12.2)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from IPython->import-ipynb) (0.4.6)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from nbformat->import-ipynb) (2.21.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from nbformat->import-ipynb) (4.23.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from nbformat->import-ipynb) (5.7.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from jedi>=0.16->IPython->import-ipynb) (0.8.4)\n",
      "Requirement already satisfied: attrs>=22.2.0 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from jsonschema>=2.6->nbformat->import-ipynb) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from jsonschema>=2.6->nbformat->import-ipynb) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from jsonschema>=2.6->nbformat->import-ipynb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from jsonschema>=2.6->nbformat->import-ipynb) (0.24.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->import-ipynb) (3.10.0)\n",
      "Requirement already satisfied: pywin32>=300 in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->import-ipynb) (308)\n",
      "Requirement already satisfied: wcwidth in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->IPython->import-ipynb) (0.2.5)\n",
      "Requirement already satisfied: executing in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from stack-data->IPython->import-ipynb) (0.8.3)\n",
      "Requirement already satisfied: asttokens in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from stack-data->IPython->import-ipynb) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from stack-data->IPython->import-ipynb) (0.2.2)\n",
      "Requirement already satisfied: six in d:\\anaconda\\envs\\torch_env\\lib\\site-packages (from asttokens->stack-data->IPython->import-ipynb) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install import-ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650c70e8-fcce-4b56-8512-1f84f552562f",
   "metadata": {},
   "source": [
    "# YOLO目标检测的推理（预测）脚本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99cbab9-8536-40aa-a777-5a574fc8a5f7",
   "metadata": {},
   "source": [
    "用于将训练好的模型加载进来，对输入的图片进行目标检测预测、绘制框、输出类别和置信度等操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "258910ec-c419-4569-bc38-a3db6d98f859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorsys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import ImageDraw, ImageFont\n",
    "import import_ipynb\n",
    "from darknet import YoloBody\n",
    "from utils import (cvtColor, get_anchors, get_classes, preprocess_input, resize_image, show_config)\n",
    "from darknet import DecodeBox\n",
    "\n",
    "class YOLO(object):\n",
    "    _defaults = {\n",
    "        # The model_path points to the weight file under the logs folder, and classes_path points to the .txt file under model_data.\n",
    "        # After training, there will be multiple weight files in the logs folder. Choose the one with the lowest validation loss.\n",
    "        # A lower validation loss does not necessarily indicate a higher mAP; it only means that the weights generalize better on the validation set.\n",
    "        \"model_path\"        : 'logs/best_epoch_weights.pth',\n",
    "        \"classes_path\"      : 'model_data/voc_classes.txt',\n",
    "        # anchors_path refers to the .txt file containing the anchor boxes and is generally not modified.\n",
    "        # anchors_mask helps the code locate the corresponding anchor boxes and is also generally not modified.\n",
    "        \"anchors_path\"      : 'model_data/yolo_anchors.txt',\n",
    "        \"anchors_mask\"      : [[6, 7, 8], [3, 4, 5], [0, 1, 2]],\n",
    "        # The input image size must be a multiple of 32.\n",
    "        \"input_shape\"       : [416, 416],\n",
    "        # Only the predicted boxes with a score higher than the confidence threshold will be kept.\n",
    "        \"confidence\"        : 0.5,\n",
    "        # The nms_iou value used for Non-Maximum Suppression (NMS).\n",
    "        \"nms_iou\"           : 0.3,\n",
    "        # This variable is used to control whether to use letterbox_image for distortion-free resizing of the input image.\n",
    "        # After multiple tests, it was found that turning off letterbox_image and using direct resizing yields better results.\n",
    "        \"letterbox_image\"   : False,\n",
    "        # Whether to use CUDA.\n",
    "        \"cuda\"              : True,\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def get_defaults(cls, n): \n",
    "        if n in cls._defaults:\n",
    "            return cls._defaults[n] # If the parameter name (string) n exists in the class variable _defaults, return the corresponding value.\n",
    "        else:\n",
    "            return \"Unrecognized attribute name '\" + n + \"'\" # If n is not in _defaults, return an error message indicating that the parameter name is unrecognized.\n",
    "\n",
    "\n",
    "    #   初始化YOLO\n",
    "    def __init__(self, **kwargs): \n",
    "        self.__dict__.update(self._defaults)\n",
    "        for name, value in kwargs.items():\n",
    "            setattr(self, name, value)\n",
    "            self._defaults[name] = value \n",
    "            \n",
    "        # Get the number of classes and anchor boxes.\n",
    "        self.class_names, self.num_classes  = get_classes(self.classes_path) \n",
    "        self.anchors, self.num_anchors      = get_anchors(self.anchors_path) \n",
    "        self.bbox_util                      = DecodeBox(self.anchors, self.num_classes, (self.input_shape[0], self.input_shape[1]), self.anchors_mask)\n",
    "\n",
    "        # Set different colors for drawing bounding boxes.\n",
    "        hsv_tuples = [(x / self.num_classes, 1., 1.) for x in range(self.num_classes)] \n",
    "        self.colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples)) \n",
    "        self.colors = list(map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), self.colors)) \n",
    "        self.generate()\n",
    "        \n",
    "        show_config(**self._defaults)\n",
    "\n",
    "    # Generate the model.\n",
    "    def generate(self, onnx=False):\n",
    "        # Build the YOLOv3 model and load the weights for the YOLOv3 model.\n",
    "        self.net    = YoloBody(self.anchors_mask, self.num_classes) \n",
    "        device      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.net.load_state_dict(torch.load(self.model_path, map_location=device)) \n",
    "        self.net    = self.net.eval() # This will disable layers like Dropout and BatchNorm, which are only used during training. This line must be added during the inference phase.\n",
    "        print('{} model, anchors, and classes loaded.'.format(self.model_path)) \n",
    "        if not onnx:\n",
    "            if self.cuda:\n",
    "                self.net = nn.DataParallel(self.net)\n",
    "                self.net = self.net.cuda() \n",
    "                \n",
    "    # Detect images.\n",
    "    def detect_image(self, image, crop = False, count = False):\n",
    "        image_shape = np.array(np.shape(image)[0:2])\n",
    "        # Convert the image to RGB here to prevent errors during prediction when using grayscale images.\n",
    "        # The code only supports prediction on RGB images, so all other types of images will be converted to RGB.\n",
    "        image       = cvtColor(image)\n",
    "        # Add gray bars to the image to achieve distortion-free resizing.\n",
    "        # Alternatively, the image can be directly resized for recognition.\n",
    "        image_data  = resize_image(image, (self.input_shape[1],self.input_shape[0]), self.letterbox_image)\n",
    "        # Add the batch_size dimension.\n",
    "        image_data  = np.expand_dims(np.transpose(preprocess_input(np.array(image_data, dtype='float32')), (2, 0, 1)), 0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            images = torch.from_numpy(image_data)\n",
    "            if self.cuda:\n",
    "                images = images.cuda()\n",
    "            # Feed the image into the network for prediction!\n",
    "            outputs = self.net(images)\n",
    "            outputs = self.bbox_util.decode_box(outputs)\n",
    "            # Stack the predicted boxes and then perform Non-Maximum Suppression (NMS).\n",
    "            results = self.bbox_util.non_max_suppression(torch.cat(outputs, 1), self.num_classes, self.input_shape, \n",
    "                        image_shape, self.letterbox_image, conf_thres = self.confidence, nms_thres = self.nms_iou)\n",
    "                                                    \n",
    "            if results[0] is None: \n",
    "                return image\n",
    "\n",
    "            top_label   = np.array(results[0][:, 6], dtype = 'int32') \n",
    "            top_conf    = results[0][:, 4] * results[0][:, 5]\n",
    "            top_boxes   = results[0][:, :4]\n",
    "        # Set the font and border thickness.\n",
    "        font        = ImageFont.truetype(font='model_data/simhei.ttf', size=np.floor(3e-2 * image.size[1] + 0.5).astype('int32'))\n",
    "        thickness   = int(max((image.size[0] + image.size[1]) // np.mean(self.input_shape), 1))\n",
    "        \n",
    "        # Draw on the image.\n",
    "        for i, c in list(enumerate(top_label)):\n",
    "            predicted_class = self.class_names[int(c)]\n",
    "            box             = top_boxes[i]\n",
    "            score           = top_conf[i]\n",
    "\n",
    "            top, left, bottom, right = box\n",
    "\n",
    "            top     = max(0, np.floor(top).astype('int32'))\n",
    "            left    = max(0, np.floor(left).astype('int32'))\n",
    "            bottom  = min(image.size[1], np.floor(bottom).astype('int32'))\n",
    "            right   = min(image.size[0], np.floor(right).astype('int32'))\n",
    "\n",
    "            label = '{} {:.2f}'.format(predicted_class, score)\n",
    "            draw = ImageDraw.Draw(image)\n",
    "            # label_size = draw.textsize(label, font)\n",
    "            bbox = draw.textbbox((0, 0), label, font=font)\n",
    "            label_size = (bbox[2] - bbox[0], bbox[3] - bbox[1])\n",
    "            label = label.encode('utf-8')\n",
    "            print(label, top, left, bottom, right)\n",
    "            \n",
    "            if top - label_size[1] >= 0:\n",
    "                text_origin = np.array([left, top - label_size[1]])\n",
    "            else:\n",
    "                text_origin = np.array([left, top + 1])\n",
    "\n",
    "            for i in range(thickness):\n",
    "                draw.rectangle([left + i, top + i, right - i, bottom - i], outline=self.colors[c])\n",
    "            draw.rectangle([tuple(text_origin), tuple(text_origin + label_size)], fill=self.colors[c])\n",
    "            draw.text(text_origin, str(label,'UTF-8'), fill=(0, 0, 0), font=font)\n",
    "            del draw\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb0842b-08e9-4f1b-aa91-dd17f1221192",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch (GPU)",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
