{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2acd24e1-16c6-48c6-9b87-52c1787af8c2",
   "metadata": {},
   "source": [
    "# DarkNet-53 主干网络实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fde377b-a17a-49a9-988f-6b15e5f34bbe",
   "metadata": {},
   "source": [
    "实现了一个简洁版的 DarkNet-53 主干网络。通过5个阶段的下采样和残差块堆叠，有效提取不同尺度的图像特征，增强网络的表达能力。残差连接设计缓解了梯度消失问题，提高了深层网络的训练稳定性。输出的多尺度特征图可以用于后续的目标检测任务，如 YOLO 系列模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af54deb3-9aec-4323-9d6a-079313bc9c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import OrderedDict\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# Residual structure\n",
    "# Use a 1x1 convolution to reduce the number of channels, then use a 3x3 convolution to extract features and increase the number of channels,\n",
    "# and finally add a residual connection.\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, inplanes, planes): # inplanes输入特征图的通道数。\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1  = nn.Conv2d(inplanes, planes[0], kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn1    = nn.BatchNorm2d(planes[0])\n",
    "        self.relu1  = nn.LeakyReLU(0.1)\n",
    "        \n",
    "        self.conv2  = nn.Conv2d(planes[0], planes[1], kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2    = nn.BatchNorm2d(planes[1])\n",
    "        self.relu2  = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu2(out)\n",
    "\n",
    "        out += residual\n",
    "        return out\n",
    "\n",
    "class DarkNet(nn.Module): \n",
    "    def __init__(self, layers): \n",
    "        super(DarkNet, self).__init__() # The layers parameter indicates the number of residual blocks in each stage.\n",
    "        self.inplanes = 32 \n",
    "        # 416,416,3 -> 416,416,32\n",
    "        self.conv1  = nn.Conv2d(3, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False) \n",
    "        # Kernel size is 3x3, stride is 1, and padding is 1, which keeps the image size unchanged.\n",
    "        self.bn1    = nn.BatchNorm2d(self.inplanes)\n",
    "        self.relu1  = nn.LeakyReLU(0.1) \n",
    "\n",
    "        # 416,416,32 -> 208,208,64\n",
    "        self.layer1 = self._make_layer([32, 64], layers[0]) # Build the first stage (increase from 32 channels to 64 channels, stacking 1 residual block).\n",
    "        # 208,208,64 -> 104,104,128\n",
    "        self.layer2 = self._make_layer([64, 128], layers[1]) # Similarly, continue building stages 2, 3, 4, and 5. Each time: the number of channels doubles, the image size is halved, and the number of residual blocks varies.\n",
    "        # 104,104,128 -> 52,52,256\n",
    "        self.layer3 = self._make_layer([128, 256], layers[2])\n",
    "        # 52,52,256 -> 26,26,512\n",
    "        self.layer4 = self._make_layer([256, 512], layers[3])\n",
    "        # 26,26,512 -> 13,13,1024\n",
    "        self.layer5 = self._make_layer([512, 1024], layers[4])\n",
    "\n",
    "        self.layers_out_filters = [64, 128, 256, 512, 1024] # Record the number of output channels in the feature map at each stage of the five main modules.\n",
    "\n",
    "        # Parameter weight initialization.\n",
    "        for m in self.modules():  \n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels \n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n)) \n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1) \n",
    "                m.bias.data.zero_() \n",
    "    \n",
    " # In each layer, first perform downsampling using a 3x3 convolution with a stride of 2, then stack residual structures.\n",
    "    def _make_layer(self, planes, blocks): \n",
    "        layers = [] \n",
    "        # Downsampling with a stride of 2 and a kernel size of 3.\n",
    "        layers.append((\"ds_conv\", nn.Conv2d(self.inplanes, planes[1], kernel_size=3, stride=2, padding=1, bias=False)))\n",
    "        layers.append((\"ds_bn\", nn.BatchNorm2d(planes[1])))\n",
    "        layers.append((\"ds_relu\", nn.LeakyReLU(0.1))) \n",
    "        # Add the residual structure.\n",
    "        self.inplanes = planes[1] # Update self.inplanes, indicating that the input channel number for stacking residual blocks in the next stage has also changed to planes[1].\n",
    "        for i in range(0, blocks):\n",
    "            layers.append((\"residual_{}\".format(i), BasicBlock(self.inplanes, planes))) \n",
    "        return nn.Sequential(OrderedDict(layers))\n",
    "\n",
    "    def forward(self, x): # Define the forward propagation process of the entire DarkNet network.\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        out3 = self.layer3(x) # Save the output as out3.\n",
    "        out4 = self.layer4(out3)\n",
    "        out5 = self.layer5(out4)\n",
    "\n",
    "        return out3, out4, out5\n",
    "\n",
    "def darknet53():\n",
    "    model = DarkNet([1, 2, 8, 8, 4])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd8a653-ade1-4fcb-b1eb-75a09679b4f9",
   "metadata": {},
   "source": [
    "# YOLOv3 检测头完整实现：结合 Darknet53 主干网络与多尺度特征融合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bc88e4-5b15-4222-874b-68f4842d9e45",
   "metadata": {},
   "source": [
    "基于 Darknet-53 主干网络实现了 YOLOv3 的完整检测头模块，包括3个尺度的特征提取和预测输出。通过卷积模块、上采样和特征融合策略，有效提升了检测不同尺寸目标的能力。输出的三组特征图分别对应大、中、小物体检测，支撑后续边界框回归和分类任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f7a2b3c-c281-48a2-97aa-edc46ad9d8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(filter_in, filter_out, kernel_size): # Define a convolutional block function where the input channels are filter_in, the output channels are filter_out, and the kernel size is kernel_size.\n",
    "    pad = (kernel_size - 1) // 2 if kernel_size else 0 # Calculate the amount of padding in order to ensure that the feature map size remains unchanged after convolution.\n",
    "    return nn.Sequential(OrderedDict([ \n",
    "        (\"conv\", nn.Conv2d(filter_in, filter_out, kernel_size=kernel_size, stride=1, padding=pad, bias=False)), \n",
    "        (\"bn\", nn.BatchNorm2d(filter_out)), \n",
    "        (\"relu\", nn.LeakyReLU(0.1)), \n",
    "    ]))\n",
    "\n",
    "# In make_last_layers, there are a total of seven convolutions.\n",
    "# The first five are used for feature extraction, and the last two are used to obtain the YOLO network's prediction results.\n",
    "def make_last_layers(filters_list, in_filters, out_filter): \n",
    "    # Pass in three parameters:\n",
    "    # filters_list: a list, such as [512, 1024], specifying the intermediate number of channels.\n",
    "    # in_filters: the number of input feature map channels, e.g., 1024.\n",
    "    # out_filter: the number of output feature map channels (usually calculated as the number of boxes per grid × number of classes + 5).\n",
    "    m = nn.Sequential(\n",
    "        conv2d(in_filters, filters_list[0], 1), \n",
    "        conv2d(filters_list[0], filters_list[1], 3), \n",
    "        conv2d(filters_list[1], filters_list[0], 1),\n",
    "        conv2d(filters_list[0], filters_list[1], 3),\n",
    "        conv2d(filters_list[1], filters_list[0], 1),\n",
    "        conv2d(filters_list[0], filters_list[1], 3),\n",
    "        nn.Conv2d(filters_list[1], out_filter, kernel_size=1, stride=1, padding=0, bias=True) # Actually generate the final output tensor, where each grid cell predicts bounding boxes and class probabilities.\n",
    "    )\n",
    "    return m \n",
    "\n",
    "class YoloBody(nn.Module):\n",
    "    def __init__(self, anchors_mask, num_classes, pretrained = False): \n",
    "        # anchors_mask: which anchors are used at each layer\n",
    "        # num_classes: the number of object classes to detect\n",
    "        # pretrained=False: whether to load pretrained Darknet-53 weights\n",
    "        super(YoloBody, self).__init__()\n",
    "        # Generate the Darknet-53 backbone model\n",
    "        # Obtain three valid feature layers with shapes:\n",
    "        # 52×52×256\n",
    "        # 26×26×512\n",
    "        # 13×13×1024\n",
    "        self.backbone = darknet53() \n",
    "        if pretrained:\n",
    "            self.backbone.load_state_dict(torch.load(\"model_data/darknet53_backbone_weights.pth\")) \n",
    "        #   out_filters : [64, 128, 256, 512, 1024]\n",
    "        out_filters = self.backbone.layers_out_filters # out_filters stores the number of output channels for each stage.\n",
    "\n",
    "        #   Calculate the number of output channels in yolo_head for the VOC dataset:\n",
    "        #   final_out_filter0 = final_out_filter1 = final_out_filter2 = 75\n",
    "        self.last_layer0            = make_last_layers([512, 1024], out_filters[-1], len(anchors_mask[0]) * (num_classes + 5))\n",
    "        # First YOLO detection head (handles the feature map with the smallest resolution):\n",
    "        # Input channels: out_filters[-1] = 1024\n",
    "        # Intermediate channel list: [512, 1024]\n",
    "        # Output channels: number of anchors per grid cell × (number of classes + 5)\n",
    "        self.last_layer1_conv       = conv2d(512, 256, 1)\n",
    "        self.last_layer1_upsample   = nn.Upsample(scale_factor=2, mode='nearest') \n",
    "        self.last_layer1            = make_last_layers([256, 512], out_filters[-2] + 256, len(anchors_mask[1]) * (num_classes + 5))\n",
    "        # Second YOLO detection head (handles the medium-resolution feature map):\n",
    "        # Input channels: out_filters[-2] + 256 (original 512 + 256 from upsampling)\n",
    "        # Intermediate channel list: [256, 512]\n",
    "        # Output channels: number of anchors × (number of classes + 5)\n",
    "        self.last_layer2_conv       = conv2d(256, 128, 1) \n",
    "        self.last_layer2_upsample   = nn.Upsample(scale_factor=2, mode='nearest') \n",
    "        self.last_layer2            = make_last_layers([128, 256], out_filters[-3] + 128, len(anchors_mask[2]) * (num_classes + 5))\n",
    "        # Third YOLO detection head (handles the highest-resolution feature map):\n",
    "        # Input channels: out_filters[-3] + 128\n",
    "        # Intermediate channel list: [128, 256]\n",
    "        # Output channels: still anchor count × (number of classes + 5)\n",
    "\n",
    "        # The backbone and all three detection heads are ready.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Obtain three valid feature layers with shapes:\n",
    "        # 52×52×256\n",
    "        # 26×26×512\n",
    "        # 13×13×1024\n",
    "        x2, x1, x0 = self.backbone(x)\n",
    "\n",
    "        #---------------------------------------------------#\n",
    "        #   The first feature layer: 52×52×256.\n",
    "        #   out0 = (batch_size,255,13,13)\n",
    "        #---------------------------------------------------#\n",
    "        # 13,13,1024 -> 13,13,512 -> 13,13,1024 -> 13,13,512 -> 13,13,1024 -> 13,13,512\n",
    "        out0_branch = self.last_layer0[:5](x0)\n",
    "        out0        = self.last_layer0[5:](out0_branch)\n",
    "\n",
    "        # 13,13,512 -> 13,13,256 -> 26,26,256\n",
    "        x1_in = self.last_layer1_conv(out0_branch)\n",
    "        x1_in = self.last_layer1_upsample(x1_in)\n",
    "\n",
    "        # 26,26,256 + 26,26,512 -> 26,26,768\n",
    "        x1_in = torch.cat([x1_in, x1], 1)\n",
    "        #---------------------------------------------------#\n",
    "        #   The second feature layer: 52×52×256.\n",
    "        #   out1 = (batch_size,255,26,26)\n",
    "        #---------------------------------------------------#\n",
    "        # 26,26,768 -> 26,26,256 -> 26,26,512 -> 26,26,256 -> 26,26,512 -> 26,26,256\n",
    "        out1_branch = self.last_layer1[:5](x1_in)\n",
    "        out1        = self.last_layer1[5:](out1_branch)\n",
    "\n",
    "        # 26,26,256 -> 26,26,128 -> 52,52,128\n",
    "        x2_in = self.last_layer2_conv(out1_branch)\n",
    "        x2_in = self.last_layer2_upsample(x2_in)\n",
    "\n",
    "        # 52,52,128 + 52,52,256 -> 52,52,384\n",
    "        x2_in = torch.cat([x2_in, x2], 1)\n",
    "        #---------------------------------------------------#\n",
    "        #   The third feature layer: 52×52×256.\n",
    "        #   out3 = (batch_size,255,52,52)\n",
    "        #---------------------------------------------------#\n",
    "        # 52,52,384 -> 52,52,128 -> 52,52,256 -> 52,52,128 -> 52,52,256 -> 52,52,128\n",
    "        out2 = self.last_layer2(x2_in)\n",
    "        return out0, out1, out2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144946fc-7296-4214-aa6d-03c77b70560e",
   "metadata": {},
   "source": [
    "<center>\n",
    "  <img src=\"111.png\" width=\"200\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3390f3ff-9551-4e9d-b786-f3b5b67d836a",
   "metadata": {},
   "source": [
    "# 对 YOLO 模型输出的特征图进行解码和后处理，得到最终用于评估的目标框坐标和类别预测结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01846221-a942-41c5-804b-6213a1896292",
   "metadata": {},
   "source": [
    "实现了 YOLO 检测模型的 输出特征图解码与后处理模块，包括如下功能：\n",
    "- 多尺度特征图解码：将模型输出的特征图解码为边界框中心、宽高、物体置信度和类别概率，适配三种不同尺寸的特征图（如13×13、26×26、52×52）。\n",
    "- 先验框调整与归一化：通过网格偏移与指数变换恢复边界框真实尺寸，并将其归一化至输入尺寸比例。\n",
    "- 坐标反变换：将归一化坐标转换为原图尺寸的实际像素坐标，支持是否使用 LetterBox。\n",
    "- 非极大值抑制（NMS）：基于 PyTorch 官方的 NMS 算法，去除重复冗余框，保留置信度高、重叠小的框。\n",
    "- 最终输出格式标准化：返回每张图片的检测结果，包含边界框左上角与右下角坐标、物体置信度、类别置信度和类别编号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fd259df-953a-4b15-843d-9c1428672a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import nms\n",
    "import numpy as np\n",
    "\n",
    "class DecodeBox(): # Define a class DecodeBox that is specifically used to process the output feature maps of YOLO.\n",
    "    def __init__(self, anchors, num_classes, input_shape, anchors_mask = [[6,7,8], [3,4,5], [0,1,2]]):\n",
    "        super(DecodeBox, self).__init__()\n",
    "        self.anchors        = anchors\n",
    "        self.num_classes    = num_classes\n",
    "        self.bbox_attrs     = 5 + num_classes\n",
    "        self.input_shape    = input_shape\n",
    "        # anchors: the dimensions of all anchor boxes.\n",
    "        # num_classes: the number of object classes.\n",
    "        # input_shape: the input dimensions of the model.\n",
    "        # anchors_mask: indicates which 3 anchors are used for each scale of the feature map. For example, the 13x13 feature map uses the 6th, 7th, and 8th anchors.\n",
    "        #-----------------------------------------------------------#\n",
    "        #   The 13x13 feature layer corresponds to the anchors: [116,90], [156,198], [373,326]\n",
    "        #   The 26x26 feature layer corresponds to the anchors: [30,61], [62,45], [59,119]\n",
    "        #   The 52x52 feature layer corresponds to the anchors: [10,13], [16,30], [33,23]\n",
    "        #-----------------------------------------------------------#\n",
    "        self.anchors_mask   = anchors_mask\n",
    "\n",
    "    def decode_box(self, inputs): # Decode the model outputs into bounding box information.\n",
    "                                    # 'inputs' is a list of feature maps output by the model (corresponding to large, medium, and small scales).\n",
    "        outputs = [] \n",
    "        for i, input in enumerate(inputs): \n",
    "            #-----------------------------------------------#\n",
    "            #   The input 'inputs' contains three tensors, and their shapes are:\n",
    "            #   batch_size, 75, 13, 13\n",
    "            #   batch_size, 75, 26, 26\n",
    "            #   batch_size, 75, 52, 52\n",
    "            #-----------------------------------------------#\n",
    "            batch_size      = input.size(0)\n",
    "            input_height    = input.size(2)\n",
    "            input_width     = input.size(3)\n",
    "            # Get the batch size, width, and height of the current feature map.\n",
    "\n",
    "            # The input is a 416×416 image. If the feature map is downsampled to 13x13, \n",
    "            # it means the stride is 32.\n",
    "            stride_h = self.input_shape[0] / input_height\n",
    "            stride_w = self.input_shape[1] / input_width\n",
    "            #-------------------------------------------------#\n",
    "            # At this point, the obtained 'scaled_anchors' are sized relative to the feature layer.\n",
    "            #-------------------------------------------------#\n",
    "            scaled_anchors = [(anchor_width / stride_w, anchor_height / stride_h) for anchor_width, anchor_height in self.anchors[self.anchors_mask[i]]]\n",
    "\n",
    "            #-----------------------------------------------#\n",
    "            #   The input 'inputs' contains three tensors, and their shapes are:\n",
    "            #   batch_size, 3, 13, 13, 25\n",
    "            #   batch_size, 3, 26, 26, 25\n",
    "            #   batch_size, 3, 52, 52, 25\n",
    "            #-----------------------------------------------#\n",
    "            # Reshape the feature map to facilitate subsequent operations.\n",
    "            # The shape becomes [batch, 3, height, width, bbox_attrs].\n",
    "            prediction = input.view(batch_size, len(self.anchors_mask[i]),\n",
    "                                    self.bbox_attrs, input_height, input_width).permute(0, 1, 3, 4, 2).contiguous()\n",
    "\n",
    "            #-----------------------------------------------#\n",
    "            #   Adjustment parameters for the center position of the anchor boxes.\n",
    "            #-----------------------------------------------#\n",
    "            x = torch.sigmoid(prediction[..., 0])  \n",
    "            y = torch.sigmoid(prediction[..., 1])\n",
    "            #-----------------------------------------------#\n",
    "            #   Adjustment parameters for the width and height of the anchor boxes.\n",
    "            #-----------------------------------------------#\n",
    "            w = prediction[..., 2]\n",
    "            h = prediction[..., 3]\n",
    "            #-----------------------------------------------#\n",
    "            #   Obtain the confidence score, indicating whether there is an object.\n",
    "            #-----------------------------------------------#\n",
    "            conf        = torch.sigmoid(prediction[..., 4])\n",
    "            #-----------------------------------------------#\n",
    "            #   Class confidence scores.\n",
    "            #-----------------------------------------------#\n",
    "            # pred_cls: class probabilities.\n",
    "            pred_cls    = torch.sigmoid(prediction[..., 5:])\n",
    "            FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor\n",
    "            LongTensor  = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor\n",
    "\n",
    "            #----------------------------------------------------------#\n",
    "            #   Generate the grid: anchor box centers relative to the top-left corner of each grid cell.\n",
    "            #   Shape: batch_size, 3, 13, 13\n",
    "            #----------------------------------------------------------#\n",
    "            grid_x = torch.linspace(0, input_width - 1, input_width).repeat(input_height, 1).repeat(\n",
    "                batch_size * len(self.anchors_mask[i]), 1, 1).view(x.shape).type(FloatTensor)\n",
    "            grid_y = torch.linspace(0, input_height - 1, input_height).repeat(input_width, 1).t().repeat(\n",
    "                batch_size * len(self.anchors_mask[i]), 1, 1).view(y.shape).type(FloatTensor)\n",
    "\n",
    "            #----------------------------------------------------------#\n",
    "            #   Generate the anchor box widths and heights in grid format.\n",
    "            #   Shape: batch_size, 3, 13, 13\n",
    "            #----------------------------------------------------------#\n",
    "            anchor_w = FloatTensor(scaled_anchors).index_select(1, LongTensor([0]))\n",
    "            anchor_h = FloatTensor(scaled_anchors).index_select(1, LongTensor([1]))\n",
    "            anchor_w = anchor_w.repeat(batch_size, 1).repeat(1, 1, input_height * input_width).view(w.shape)\n",
    "            anchor_h = anchor_h.repeat(batch_size, 1).repeat(1, 1, input_height * input_width).view(h.shape)\n",
    "\n",
    "            #----------------------------------------------------------#\n",
    "            # Adjust the anchor boxes using the prediction results.\n",
    "            # First, adjust the center of the anchor boxes by offsetting from the top-left corner of the grid cell.\n",
    "            # Then, adjust the width and height of the anchor boxes.\n",
    "            #----------------------------------------------------------#\n",
    "            pred_boxes          = FloatTensor(prediction[..., :4].shape)\n",
    "            pred_boxes[..., 0]  = x.data + grid_x\n",
    "            pred_boxes[..., 1]  = y.data + grid_y\n",
    "            pred_boxes[..., 2]  = torch.exp(w.data) * anchor_w\n",
    "            pred_boxes[..., 3]  = torch.exp(h.data) * anchor_h\n",
    "\n",
    "            #----------------------------------------------------------#\n",
    "            #   Normalize the output results into decimal format.\n",
    "            #----------------------------------------------------------#\n",
    "            _scale = torch.Tensor([input_width, input_height, input_width, input_height]).type(FloatTensor)\n",
    "            output = torch.cat((pred_boxes.view(batch_size, -1, 4) / _scale,\n",
    "                                conf.view(batch_size, -1, 1), pred_cls.view(batch_size, -1, self.num_classes)), -1) # shape = (1, 507, 11)，表示1张图，共507个预测框，每个框11个值\n",
    "            outputs.append(output.data) \n",
    "        return outputs # Finally, return all 25 pieces of information for each box on the 13x13 feature map.\n",
    "\n",
    "    def yolo_correct_boxes(self, box_xy, box_wh, input_shape, image_shape, letterbox_image): # Convert the normalized decimal coordinates back to actual pixel coordinates in the original image size.\n",
    "        # box_xy: predicted center coordinates (x, y), normalized.\n",
    "        # box_wh: predicted width and height (w, h), normalized.\n",
    "        # input_shape: input size of the model.\n",
    "        # image_shape: original image size.\n",
    "        # letterbox_image: whether letterbox (padding to keep aspect ratio) was used.\n",
    "        #-----------------------------------------------------------------#\n",
    "        # Putting the y-axis first is for easier multiplication with the image height and width.\n",
    "        #-----------------------------------------------------------------#\n",
    "        box_yx = box_xy[..., ::-1] \n",
    "        box_hw = box_wh[..., ::-1]\n",
    "        input_shape = np.array(input_shape)\n",
    "        image_shape = np.array(image_shape)\n",
    "\n",
    "        if letterbox_image: # 如果有加黑边(letterbox)，需要特殊处理：\n",
    "            #-----------------------------------------------------------------#\n",
    "            #   The offset calculated here represents the offset of the valid image area relative to the top-left corner.\n",
    "            #   'new_shape' refers to the scaled dimensions (width and height).\n",
    "            #-----------------------------------------------------------------#\n",
    "            # new_shape: the size of the original image after proportional scaling to fit into the input_shape.\n",
    "            # offset: the width of the black borders (padding) around the image after scaling.\n",
    "            # scale: the scaling ratio.\n",
    "            # Finally, remove the effect of the padding and restore to the position before scaling.\n",
    "            new_shape = np.round(image_shape * np.min(input_shape/image_shape))\n",
    "            offset  = (input_shape - new_shape)/2./input_shape\n",
    "            scale   = input_shape/new_shape\n",
    "\n",
    "            box_yx  = (box_yx - offset) * scale\n",
    "            box_hw *= scale\n",
    "        # Calculate the top-left and bottom-right coordinates:\n",
    "        # Use the center point and the width/height to derive the top-left and bottom-right points.\n",
    "        box_mins    = box_yx - (box_hw / 2.)\n",
    "        box_maxes   = box_yx + (box_hw / 2.)\n",
    "        # Concatenate to form the final boxes:\n",
    "        # Merge the coordinates into [x_min, y_min, x_max, y_max].\n",
    "        # Then multiply by the original image size to convert to actual pixel values.\n",
    "        boxes  = np.concatenate([box_mins[..., 0:1], box_mins[..., 1:2], box_maxes[..., 0:1], box_maxes[..., 1:2]], axis=-1)\n",
    "        boxes *= np.concatenate([image_shape, image_shape], axis=-1) \n",
    "        return boxes # Return the bounding box coordinates in pixel values corresponding to the original image size.\n",
    "\n",
    "    def non_max_suppression(self, prediction, num_classes, input_shape, image_shape, letterbox_image, conf_thres=0.5, nms_thres=0.4): # Filter predicted boxes based on confidence scores, and remove duplicate boxes with too much overlap.\n",
    "        # conf_thres: confidence threshold; discard boxes with scores below this.\n",
    "        # nms_thres: threshold for Non-Maximum Suppression (NMS).\n",
    "        #----------------------------------------------------------#\n",
    "        #   Convert the format of the prediction results to [top-left, bottom-right] format.\n",
    "        #   prediction shape: [batch_size, num_anchors, 25]\n",
    "        #----------------------------------------------------------#\n",
    "        box_corner          = prediction.new(prediction.shape)\n",
    "        box_corner[:, :, 0] = prediction[:, :, 0] - prediction[:, :, 2] / 2\n",
    "        box_corner[:, :, 1] = prediction[:, :, 1] - prediction[:, :, 3] / 2\n",
    "        box_corner[:, :, 2] = prediction[:, :, 0] + prediction[:, :, 2] / 2\n",
    "        box_corner[:, :, 3] = prediction[:, :, 1] + prediction[:, :, 3] / 2\n",
    "        prediction[:, :, :4] = box_corner[:, :, :4] # Assign the coordinates from the first 4 columns of box_corner ([x1, y1, x2, y2]) \n",
    "                                                    # back to the first 4 columns of prediction.\n",
    "\n",
    "        output = [None for _ in range(len(prediction))]\n",
    "        for i, image_pred in enumerate(prediction): \n",
    "            # prediction is a tensor of shape [batch_size, num_anchors, 5 + num_classes].\n",
    "            # enumerate(prediction) processes the prediction results for each image.\n",
    "            # image_pred has the shape [num_anchors, 5 + num_classes], representing the prediction info of all anchors for that image.\n",
    "            #----------------------------------------------------------#\n",
    "            #   Take the max value over the class prediction part.\n",
    "            #   class_conf: [num_anchors, 1]    class confidence\n",
    "            #   class_pred: [num_anchors, 1]    predicted class\n",
    "            #----------------------------------------------------------#\n",
    "            class_conf, class_pred = torch.max(image_pred[:, 5:5 + num_classes], 1, keepdim=True) \n",
    "            #----------------------------------------------------------#\n",
    "            #   Use the confidence scores to perform the first round of filtering.\n",
    "            #----------------------------------------------------------#\n",
    "            conf_mask = (image_pred[:, 4] * class_conf[:, 0] >= conf_thres).squeeze()\n",
    "            #----------------------------------------------------------#\n",
    "            #   Filter the prediction results based on confidence scores.\n",
    "            #----------------------------------------------------------#\n",
    "            image_pred = image_pred[conf_mask]\n",
    "            class_conf = class_conf[conf_mask]\n",
    "            class_pred = class_pred[conf_mask]\n",
    "            if not image_pred.size(0):\n",
    "                continue\n",
    "            #-------------------------------------------------------------------------#\n",
    "            #   detections: [num_anchors, 7]\n",
    "            #   The 7 elements are: x1, y1, x2, y2, object confidence, class confidence, predicted class\n",
    "            #-------------------------------------------------------------------------#\n",
    "            detections = torch.cat((image_pred[:, :5], class_conf.float(), class_pred.float()), 1)\n",
    "\n",
    "            #------------------------------------------#\n",
    "            #   Get all the unique classes contained in the prediction results.\n",
    "            #------------------------------------------#\n",
    "            unique_labels = detections[:, -1].cpu().unique()\n",
    "\n",
    "            if prediction.is_cuda: \n",
    "                unique_labels = unique_labels.cuda()\n",
    "                detections = detections.cuda()\n",
    "\n",
    "            for c in unique_labels: \n",
    "                #------------------------------------------#\n",
    "                #   Get all filtered prediction results for a specific class.\n",
    "                #------------------------------------------#\n",
    "                detections_class = detections[detections[:, -1] == c]\n",
    "\n",
    "                #------------------------------------------#\n",
    "                #   Using the built-in Non-Maximum Suppression (NMS) provided by the official library is faster!\n",
    "                #------------------------------------------#\n",
    "                keep = nms(\n",
    "                    detections_class[:, :4],\n",
    "                    detections_class[:, 4] * detections_class[:, 5], \n",
    "                    nms_thres\n",
    "                )\n",
    "                # detections_class[:, :4]: extract the box coordinates [x1, y1, x2, y2]\n",
    "                # detections_class[:, 4] * detections_class[:, 5]:\n",
    "                # objectness × class_conf, used as the overall confidence score for NMS\n",
    "                # Boxes with higher confidence are kept first\n",
    "                # nms_thres: IOU threshold for NMS, default is 0.4 (if two boxes overlap more than 0.4, the one with lower score is removed)\n",
    "                max_detections = detections_class[keep] \n",
    "                \n",
    "                # Add max detections to outputs \n",
    "                output[i] = max_detections if output[i] is None else torch.cat((output[i], max_detections)) \n",
    "            \n",
    "            if output[i] is not None:# Finally, process each image: convert the predicted boxes from relative coordinates back to original image pixel coordinates.\n",
    "                output[i]           = output[i].cpu().numpy()\n",
    "                box_xy, box_wh      = (output[i][:, 0:2] + output[i][:, 2:4])/2, output[i][:, 2:4] - output[i][:, 0:2] # Convert each box format from [x1, y1, x2, y2] (top-left ➜ bottom-right) back to center point and width-height format:\n",
    "                # box_xy is the center point position (x_center, y_center)\n",
    "                # ➜ (x1 + x2)/2, (y1 + y2)/2\n",
    "                # box_wh is the width and height (w, h)\n",
    "                # ➜ x2 - x1, y2 - y1\n",
    "                # This step is to prepare for the next function yolo_correct_boxes(), which requires center point and width-height as input.\n",
    "\n",
    "                output[i][:, :4]    = self.yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape, letterbox_image)\n",
    "\n",
    "        return output \n",
    "        # output is a list with a length equal to batch_size. It contains all the decoded predicted boxes\n",
    "        # from each feature layer for every input image!\n",
    "        # Each element is a NumPy array with shape [num_detections, 7]\n",
    "        # Each row is: [x1, y1, x2, y2, obj_conf, class_conf, class_id]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch (GPU)",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
